{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793ea6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82d3e30",
   "metadata": {},
   "source": [
    "# Hunalign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5b1cfa",
   "metadata": {},
   "source": [
    "### Text preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7346396",
   "metadata": {},
   "source": [
    "English text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2587ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    en_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e8b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ready = []\n",
    "for i in sent_tokenize(en_text):\n",
    "    en_ready.append('<p>')\n",
    "    en_ready.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618606f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en_hunalign.txt', 'w', encoding='utf-8-sig') as f:\n",
    "    f.write('\\n'.join(en_ready))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2cdde0",
   "metadata": {},
   "source": [
    "Russian text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ru.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    ru_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf05c6cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ru_ready = []\n",
    "for i in sent_tokenize(ru_text):\n",
    "    new_i = re.sub('([.,!?();])', r' \\1 ', i)\n",
    "    new_i = re.sub('\\s{2,}', ' ', new_i)\n",
    "    ru_ready.append('<p>')\n",
    "    ru_ready.append(new_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7872cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ru_hunalign.txt', 'w', encoding='utf-8-sig') as f:\n",
    "    f.write('\\n'.join(ru_ready))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1cc3e9",
   "metadata": {},
   "source": [
    "### Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fbc803",
   "metadata": {},
   "source": [
    "В папку с приложением hunalign.exe и нулевым словарем null.dic (так как для русского языка нет готового словаря) положить файлы en_hunalign и ru_hunalign, в которых находятся в готовом для элайнмента формате предложения на соответствующих языках, открыть в ней командную строку и ввести следующее (результат будет в файле hunalign.txt):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843b667b",
   "metadata": {},
   "source": [
    "hunalign.exe null.dic en_hunalign.txt ru_hunalign.txt -text -utf -realign > hunalign.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a094ea24",
   "metadata": {},
   "source": [
    "### Hunalign table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7981b377",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hunalign.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    aligned_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a5820",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_p = aligned_text.split('\\n')\n",
    "\n",
    "without_p = []\n",
    "for i in range(1, len(with_p), 2):\n",
    "    without_p.append(with_p[i])\n",
    "\n",
    "en_hunalign_df = []\n",
    "for element in without_p:\n",
    "    for en in range(0, len(element.split('\\t')), 3):\n",
    "        en_hunalign_df.append(element.split('\\t')[en])\n",
    "\n",
    "ru_hunalign_df = []\n",
    "for element in without_p:\n",
    "    for ru in range(1, len(element.split('\\t')), 3):\n",
    "        ru_hunalign_df.append(element.split('\\t')[ru])\n",
    "\n",
    "score = []\n",
    "for element in without_p:\n",
    "    for sc in range(2, len(element.split('\\t')), 3):\n",
    "        score.append(element.split('\\t')[sc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da17adac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hunalign = pd.DataFrame({'en': en_hunalign_df, 'ru': ru_hunalign_df, 'score': score})\n",
    "\n",
    "df_hunalign.to_excel('./hunalign.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2472ca9e",
   "metadata": {},
   "source": [
    "### Hunalign table with entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6513932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "\n",
    "for count, letter in enumerate(en_text):\n",
    "    if en_text[count-1]+letter == ' .' or letter == '?' or letter == '!':\n",
    "        points.append(count)\n",
    "\n",
    "for i, value in enumerate(en_hunalign_df):\n",
    "    if value == '':\n",
    "        points.insert(i, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab14b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entities = pd.read_excel('entities.xlsx', index_col=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7f8e36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_hunalign_entities = pd.DataFrame({'en': en_hunalign_df, 'ru': ru_hunalign_df, 'points': points, 'entities': ''*len(points)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf001c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "entities = [[] for _ in range(len(points))]\n",
    "\n",
    "for i, value in enumerate(df_entities[1]):\n",
    "    for ind, p in enumerate(points):\n",
    "        if p != '' and int(value.split(' ')[1]) < int(p):\n",
    "            entities[[df_hunalign_entities[df_hunalign_entities['points']==p].index.values][0][0]].append(str(value.split(' ')[0])+'-'+df_entities[2][i])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bf461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hunalign_entities = pd.DataFrame({'en': en_hunalign_df, 'ru': ru_hunalign_df, 'entities': entities})\n",
    "\n",
    "df_hunalign_entities.to_excel('./hunalign_entities.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caae2d4",
   "metadata": {},
   "source": [
    "# Fast_align"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce870750",
   "metadata": {},
   "source": [
    "### Text preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e72757",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_en_text = []\n",
    "for sent in sent_tokenize(en_text.lower()):\n",
    "    tokenized_en_text.append(tokenizer.tokenize(sent))\n",
    "    \n",
    "clean_en_text = []\n",
    "for sent in tokenized_en_text:\n",
    "    clean_en_text.append(' '.join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1fd23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ru_text = []\n",
    "for sent in sent_tokenize(ru_text.lower(), language=\"russian\"):\n",
    "    tokenized_ru_text.append(tokenizer.tokenize(sent))\n",
    "\n",
    "clean_ru_text = []\n",
    "for sent in tokenized_ru_text:\n",
    "    clean_ru_text.append(' '.join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4aefb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en_fastalign.txt', 'w', encoding='utf-8-sig') as f:\n",
    "    f.write('\\n'.join(clean_en_text))\n",
    "\n",
    "with open('ru_fastalign.txt', 'w', encoding='utf-8-sig') as f:\n",
    "    f.write('\\n'.join(clean_ru_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d5fb92",
   "metadata": {},
   "source": [
    "### Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bd0191",
   "metadata": {},
   "source": [
    "Через консоль Ubuntu. Сначала приведем в нужный формат по предложениям:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88eb9d9",
   "metadata": {},
   "source": [
    "paste en_fastalign.txt ru_fastalign.txt | sed \"s/$(printf '\\t')/ ||| /g\" > source_targets.fastalign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc59c1",
   "metadata": {},
   "source": [
    "Сначала мы рассматриваем английский язык как мишень (target), а русский язык как источник (source), поэтому используем reverse:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b30f1a",
   "metadata": {},
   "source": [
    "./fast_align -i source_targets.fastalign -d -o -v -r > reverse.align"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70bc80",
   "metadata": {},
   "source": [
    "### Fast_align table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3d12e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fastalign_en_ru.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    aligned_words = f.read()\n",
    "\n",
    "aligned_words = aligned_words.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e93403",
   "metadata": {},
   "outputs": [],
   "source": [
    "sootnosh = []\n",
    "for i in range(len(aligned_words)-1):\n",
    "    sootnosh.append([[aligned_words[i]],[clean_en_text[i], clean_ru_text[i]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c57aab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_fastalign_incomplete = pd.DataFrame({'en': [], 'ru': []}) #поменять местами для ru_en\n",
    "\n",
    "k = 0\n",
    "for soot in sootnosh:\n",
    "    indexes = soot[0][0].split(' ')\n",
    "    list_en = []\n",
    "    list_ru = []\n",
    "    \n",
    "    for i in indexes:\n",
    "        if soot[0][0] == '':\n",
    "            for j in range(len(soot[1][0].split(' '))):\n",
    "                list_en.append(soot[1][0].split(' ')[j])\n",
    "                list_ru.append('')\n",
    "        else:\n",
    "            list_en.append(soot[1][0].split(' ')[int(i.split('-')[0])])\n",
    "            list_ru.append(soot[1][1].split(' ')[int(i.split('-')[1])])     \n",
    "        \n",
    "    inde = [k]\n",
    "    df2 = pd.DataFrame(list(zip(inde, inde)), columns=['en', 'ru'])\n",
    "    df_fastalign_incomplete = df_fastalign_incomplete.append(df2)\n",
    "    df2 = pd.DataFrame(list(zip(list_en, list_ru)), columns=['en', 'ru']) # поменять местами list_en и list_ru для ru_en и columns\n",
    "    df_fastalign_incomplete = df_fastalign_incomplete.append(df2)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3415b26",
   "metadata": {},
   "source": [
    "### Fast_align complete table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_fastalign_df = []\n",
    "for i in range(len(en_hunalign_df)):\n",
    "    new_str = str(i) + ' '\n",
    "    en_fastalign_df.append(new_str + en_hunalign_df[i])\n",
    "\n",
    "sentences = []\n",
    "for i in en_fastalign_df:\n",
    "    new_i = i.split(' ')\n",
    "    sentences.append(new_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648c5a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_sentences = []\n",
    "incomplete_one_sent = []\n",
    "\n",
    "len_k = int(sentences[-1][0]) + 1\n",
    "k = 0\n",
    "for i in df_fastalign_incomplete['en']:\n",
    "    if i == k + 1:\n",
    "        incomplete_sentences.append(incomplete_one_sent)\n",
    "        incomplete_one_sent = []\n",
    "        k += 1\n",
    "        incomplete_one_sent.append(i)\n",
    "    else:\n",
    "        incomplete_one_sent.append(str(i))\n",
    "    \n",
    "incomplete_sentences.append(incomplete_one_sent)\n",
    "incomplete_sentences[0][0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fb68a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pair = []\n",
    "incomplete = []\n",
    "\n",
    "k = 1\n",
    "minus_len = 0\n",
    "for i, value in enumerate(df_fastalign_incomplete['ru']):\n",
    "    if value == k:\n",
    "        incomplete.append(pair)\n",
    "        pair = []\n",
    "        minus_len += len(incomplete[k-1])\n",
    "        k += 1\n",
    "        \n",
    "    pair.append(str(incomplete_sentences[k-1][i-minus_len]) + '-' + str(value))\n",
    "\n",
    "incomplete.append(pair)\n",
    "incomplete[0][0] = '0-0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c78abd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "complete = []\n",
    "\n",
    "for one_sent in sentences:\n",
    "    j = 0\n",
    "    pair = sentences.index(one_sent)\n",
    "    \n",
    "    for i in range(len(one_sent)):\n",
    "        \n",
    "        try:\n",
    "            if one_sent[i].lower() == incomplete[pair][j].split('-')[0]:\n",
    "                complete.append(incomplete[pair][j])\n",
    "                j += 1\n",
    "            elif one_sent[i].lower() == \"'s\" or one_sent[i].lower() == \"'d\":\n",
    "                complete.append(incomplete[pair][j])\n",
    "                j += 1\n",
    "            else:\n",
    "                complete.append(one_sent[i].lower() + '-' + ' ')\n",
    "        \n",
    "        except IndexError:\n",
    "            complete.append(one_sent[i].lower() + '-' + ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4904d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_fastalign_complete = []\n",
    "ru_fastalign_complete = []\n",
    "\n",
    "k = 0\n",
    "for pair in complete:\n",
    "    if pair.split('-')[0] == str(k) and pair.split('-')[1] == str(k):\n",
    "        en_fastalign_complete.append(int(pair.split('-')[0]))\n",
    "        ru_fastalign_complete.append(int(pair.split('-')[1]))\n",
    "        k += 1\n",
    "    else:\n",
    "        en_fastalign_complete.append(pair.split('-')[0])\n",
    "        ru_fastalign_complete.append(pair.split('-')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3de88f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_fastalign_complete = pd.DataFrame({'en': en_fastalign_complete, 'ru': ru_fastalign_complete})\n",
    "df_fastalign_complete.to_excel('./fastalign_en_ru_complete.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca9818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# для ru_en\n",
    "df_fastalign_complete = pd.DataFrame({'ru': ru_fastalign_complete, 'en': en_fastalign_complete})\n",
    "df_fastalign_complete.to_excel('./fastalign_ru_en_complete.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc1d867",
   "metadata": {},
   "source": [
    "### Fast_align table with entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170590dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_tags = []\n",
    "\n",
    "k = 0\n",
    "full_word = ''\n",
    "for i, word in enumerate(df_fastalign_complete['en']):\n",
    "    \n",
    "    if type(word) != str and math.isnan(word):\n",
    "        word = ''\n",
    "        len_token = 2\n",
    "    elif word == 's' or word == 'd':\n",
    "        len_token = 2\n",
    "    else:\n",
    "        len_token = len(str(word))\n",
    "   \n",
    "    try:\n",
    "        k += len_token + 1\n",
    "        if (word == '.' or word == '!' or word == '?') and (len(str(df_fastalign_complete['en'][i + 1])) == 1 and str(df_fastalign_complete['en'][i + 1]).isdigit()):\n",
    "            k -= 2\n",
    "        elif (word == '.' or word == '!' or word == '?') and (len(str(df_fastalign_complete['en'][i + 1])) == 2 and str(df_fastalign_complete['en'][i + 1]).isdigit()):\n",
    "            k -= 3 \n",
    "        elif (word == '.' or word == '!' or word == '?') and (len(str(df_fastalign_complete['en'][i + 1])) == 3 and str(df_fastalign_complete['en'][i + 1]).isdigit()):\n",
    "            k -= 4\n",
    "\n",
    "        all_tags.append(str(word) + '-' + ' ')\n",
    "        for start in df_entities[1]:\n",
    "            if k - 2 > int(start.split(' ')[1]) and k - 2 <= int(start.split(' ')[2])+1 and word != '.' and word != '!' and word != '?':\n",
    "                fin = str(word) + '-' + start.split(' ')[0]\n",
    "                \n",
    "                if all_tags[len(all_tags) - 1].split('-')[0] == word and all_tags[len(all_tags) - 1].split('-')[1] == ' ':\n",
    "                    all_tags[len(all_tags) - 1] = fin\n",
    "                elif all_tags[len(all_tags) - 1].split('-')[0] == word and all_tags[len(all_tags) - 1].split('-')[1] != ' ':\n",
    "                    all_tags[len(all_tags) - 1] = all_tags[len(all_tags) - 1] + '-' + start.split(' ')[0]\n",
    "                else:\n",
    "                    all_tags.append(fin)\n",
    "                    \n",
    "    except KeyError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4883d14c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "en_fastalign_entities = []\n",
    "fastalign_entities = []\n",
    "for i, value in enumerate(all_tags):\n",
    "    \n",
    "    en_fastalign_entities.append(value.split('-')[0])\n",
    "    if value.split('-')[1] == ' ':\n",
    "        fastalign_entities.append('')\n",
    "    else:\n",
    "        fastalign_entities.append(value.split('-')[1:])\n",
    "\n",
    "ru_fastalign_entities = list(df_fastalign_complete['ru'])\n",
    "en_fastalign_entities.append('.')\n",
    "fastalign_entities.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c81a15f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_fastalign_entities = pd.DataFrame({'en': en_fastalign_entities, 'ru': ru_fastalign_entities, 'tags': fastalign_entities})\n",
    "df_fastalign_entities.to_excel('./fastalign_en_ru_entities.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e0ac8b",
   "metadata": {},
   "source": [
    "### Ru_en fast_align"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d48554a",
   "metadata": {},
   "source": [
    "Теперь проделываем то же самое, но рассматриваем русский язык как мишень (target), а английский как источник (source), используем forward:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55280a75",
   "metadata": {},
   "source": [
    "./fast_align -i source_targets.fastalign -d -o -v > forward.align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c871556",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fastalign_ru_en.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    aligned_words = f.read()\n",
    "\n",
    "aligned_words = aligned_words.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c4eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tags = {}\n",
    "list_dict_tags = []\n",
    "\n",
    "k = 1\n",
    "for i, word in enumerate(df_fastalign_entities['en']):\n",
    "    \n",
    "    if word == str(k) and df_fastalign_entities['ru'][i] == k:\n",
    "        list_dict_tags.append(dict_tags)\n",
    "        dict_tags = {}\n",
    "        k += 1\n",
    "        \n",
    "    elif df_fastalign_entities['tags'][i] in dict_tags.keys() and type(df_fastalign_entities['tags'][i]) == str:\n",
    "        dict_tags[df_fastalign_entities['tags'][i]].append(word)\n",
    "        \n",
    "    elif type(df_fastalign_entities['tags'][i]) == str:\n",
    "        dict_tags[df_fastalign_entities['tags'][i]] = [word]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdebd07",
   "metadata": {},
   "source": [
    "Проделать sootnosh для ru_en."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d4997c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ru_fastalign_entities = []\n",
    "en_fastalign_entities = []\n",
    "fastalign_entities = []\n",
    "\n",
    "k = 0\n",
    "for i, word in enumerate(df_fastalign_incomplete['en']):\n",
    "    ru_fastalign_entities.append(df_fastalign_incomplete['ru'][i])\n",
    "    en_fastalign_entities.append(word)\n",
    "    \n",
    "    if word == k + 1 and df_fastalign_incomplete['ru'][i] == k + 1:\n",
    "        k += 1\n",
    "    \n",
    "    for key, value in list_dict_tags[k].items():\n",
    "        for v in value:\n",
    "            if word == v:\n",
    "                fastalign_entities.append(key)\n",
    "                break\n",
    "                \n",
    "    if len(en_fastalign_entities) > len(fastalign_entities):\n",
    "        fastalign_entities.append('')\n",
    "if len(en_fastalign_entities) > len(fastalign_entities):\n",
    "    fastalign_entities.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393752f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_fastalign_entities = pd.DataFrame({'ru': ru_fastalign_entities, 'en': en_fastalign_entities, 'tags': fastalign_entities})\n",
    "df_fastalign_entities.to_excel('./fastalign_ru_en_entities.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd40b0e5",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfb5fb0",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e410170",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = []\n",
    "\n",
    "k = 0\n",
    "for i, line in enumerate(df_fastalign_entities['ru']):\n",
    "\n",
    "    if str(df_fastalign_entities['tags'][i]) == 'nan':\n",
    "        tag = ''\n",
    "    else:\n",
    "        tag = df_fastalign_entities['tags'][i]\n",
    "        \n",
    "    if line == k and df_fastalign_entities['en'][i] == k:\n",
    "        dictionary.append(str(k) + '-' + str(k) + '-' + tag)\n",
    "        k += 1\n",
    "    else:\n",
    "        dictionary.append(str(line) + '-' + str(df_fastalign_entities['en'][i]) + '-' + tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0adbb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_models = ''\n",
    "for i in ru_hunalign_df:\n",
    "    for_models += i + ' '\n",
    "    \n",
    "for_models = for_models.replace('?', '.')\n",
    "for_models = for_models.replace('!', '.')\n",
    "for_models = for_models.replace('~~~', '')\n",
    "for_models = for_models.replace('. . .', '')\n",
    "\n",
    "new_for_models = []\n",
    "k = 0\n",
    "for i in for_models.split('.'):\n",
    "    new_i = re.sub(r'[.«»,\"\\'?:!;—]', '', i)\n",
    "    new_i = re.sub(r'[-]', ' ', new_i)\n",
    "    new_i = re.sub(r'[\\n\\xa0]', ' ', new_i)\n",
    "    new_i = re.sub(r'  ', ' ', new_i)\n",
    "    if k == 0:\n",
    "        new_for_models.append(str(k) + ' ' + new_i)\n",
    "    else:\n",
    "        new_for_models.append(str(k) + new_i)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76737ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for_models = ''\n",
    "for i in new_for_models:\n",
    "    for_models += '' + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1029ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fastalign_entities_complete = []\n",
    "\n",
    "k = 0\n",
    "j = 0\n",
    "for word in for_models.split(' '):\n",
    "    \n",
    "    if word.lower() != dictionary[j].split('-')[0]:\n",
    "        df_fastalign_entities_complete.append(word.lower() + '-' + '' + '-' + '')\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        df_fastalign_entities_complete.append(dictionary[j].split('-')[0] + '-' + dictionary[j].split('-')[1] + '-' + dictionary[j].split('-')[2])\n",
    "        j += 1\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34da800",
   "metadata": {},
   "source": [
    "### Natasha Slovnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f8cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from navec import Navec\n",
    "from slovnet import NER\n",
    "\n",
    "navec = Navec.load('navec_news_v1_1B_250K_300d_100q.tar')\n",
    "ner = NER.load('slovnet_ner_news_v1.tar')\n",
    "ner.navec(navec)\n",
    "\n",
    "natasha_slovnet = ner(for_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18493031",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_natasha = []\n",
    "\n",
    "length = 0\n",
    "for i in range(len(df_fastalign_entities_complete)):\n",
    "    length += len(df_fastalign_entities_complete[i].split('-')[0]) + 1\n",
    "    \n",
    "    for j in natasha_slovnet.spans:\n",
    "        tag = ''\n",
    "        if length >= j.start and length <= j.stop:\n",
    "            tag = j.type\n",
    "            break\n",
    "    \n",
    "    try:\n",
    "        new_tag = df_fastalign_entities_complete[i + 1] + '-' + tag\n",
    "        df_natasha.append(new_tag)\n",
    "    except IndexError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a674a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_full_tags = []\n",
    "en_full_tags = []\n",
    "en_tag_full_tags = []\n",
    "natasha = []\n",
    "\n",
    "for i in df_natasha:\n",
    "    ru_full_tags.append(i.split('-')[0])\n",
    "    en_full_tags.append(i.split('-')[1])\n",
    "    en_tag_full_tags.append(i.split('-')[2][2:5])\n",
    "    natasha.append(i.split('-')[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44c7322",
   "metadata": {},
   "source": [
    "### Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c761d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang='ru', processors='tokenize,ner')\n",
    "doc_stanza = nlp(for_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f28e95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stanza = []\n",
    "\n",
    "length = 0\n",
    "for i in range(len(df_fastalign_entities_complete)):\n",
    "    length += len(df_fastalign_entities_complete[i].split('-')[0]) + 1\n",
    "\n",
    "    for sent in doc_stanza.sentences:\n",
    "        for ent in sent.ents:\n",
    "            tag = ''\n",
    "            if length >= ent.start_char and length <= ent.end_char:\n",
    "                tag = ent.type\n",
    "                break\n",
    "        break\n",
    "    \n",
    "    try:\n",
    "        new_tag = df_fastalign_entities_complete[i + 1] + '-' + tag\n",
    "        df_stanza.append(new_tag)\n",
    "    except IndexError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c68859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza = []\n",
    "for i in df_stanza:\n",
    "    stanza.append(i.split('-')[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6644b62e",
   "metadata": {},
   "source": [
    "### Deeppavlov RuBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab35ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import configs, build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e84a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model = build_model(configs.ner.ner_rus_bert, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d2a27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 512\n",
    "chunks = [for_models[i:i+n] for i in range(0, len(for_models), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b1d4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_bert = []\n",
    "for i in chunks:\n",
    "    doc_bert.append(ner_model([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ee8e83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_bert = []\n",
    "for i in range(len(doc_bert)-1):\n",
    "    for j in range(len(doc_bert[i][0][0])):\n",
    "        for f in for_models.split(' '):\n",
    "            if f == doc_bert[i][0][0][j]:\n",
    "                if doc_bert[i][1][0][j] == 'O':\n",
    "                    df_bert.append(f + '-' + doc_bert[i][1][0][j])\n",
    "                else:\n",
    "                    df_bert.append(f + '-' + doc_bert[i][1][0][j][2:])\n",
    "                break\n",
    "            elif f == doc_bert[i][0][0][j] + doc_bert[i+1][0][0][0]:\n",
    "                df_bert.append(f + '-' + 'O')\n",
    "                break\n",
    "\n",
    "# отдельно рассмотреть последнее:\n",
    "for i in range(1, len(doc_bert[-1][0][0])):\n",
    "    df_bert.append(doc_bert[-1][0][0][i] + '-' + doc_bert[-1][1][0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb703a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = []\n",
    "for i in df_bert:\n",
    "    if i.split('-')[1] == 'O':\n",
    "        bert.append('')\n",
    "    else:\n",
    "        bert.append(i.split('-')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14911c1",
   "metadata": {},
   "source": [
    "### Dataset with all tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49b43cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_alltags = pd.DataFrame({'ru': ru_full_tags, 'en': en_full_tags, 'en_tag': en_tag_full_tags, 'natasha': natasha, 'stanza': stanza, 'bert': bert})\n",
    "df_alltags.to_excel('./all_entities.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82040335",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8431c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alltags = pd.read_excel('all_entities.xlsx', index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e273a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e502c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tag = []\n",
    "for i in df_alltags['en_tag']:\n",
    "    if str(i) == 'nan':\n",
    "        en_tag.append('')\n",
    "    else:\n",
    "        en_tag.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da909ff8",
   "metadata": {},
   "source": [
    "### Natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa892dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "natasha_tag = []\n",
    "for i in df_alltags['natasha']:\n",
    "    if str(i) == 'nan':\n",
    "        natasha_tag.append('')\n",
    "    else:\n",
    "        natasha_tag.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d421569",
   "metadata": {},
   "outputs": [],
   "source": [
    "natasha_en_tag = []\n",
    "natasha_natasha_tag = []\n",
    "for i in range(len(en_tag)):\n",
    "    if en_tag[i] == 'GPE':\n",
    "        natasha_en_tag.append('LOC')\n",
    "        natasha_natasha_tag.append(natasha_tag[i])\n",
    "    elif en_tag[i] != '' and natasha_tag[i] != '' and en_tag[i] != 'FAC' and en_tag[i] != 'VEH':\n",
    "        natasha_en_tag.append(en_tag[i])\n",
    "        natasha_natasha_tag.append(natasha_tag[i])\n",
    "    elif (en_tag[i] == '' and natasha_tag[i] != '') or (en_tag[i] != '' and natasha_tag[i] == '' and en_tag[i] != 'FAC' and en_tag[i] != 'VEH'):\n",
    "        natasha_en_tag.append(en_tag[i])\n",
    "        natasha_natasha_tag.append(natasha_tag[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9763abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(natasha_en_tag, natasha_natasha_tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b41428",
   "metadata": {},
   "source": [
    "### Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cfc7d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stanza_tag = []\n",
    "for i in df_alltags['stanza']:\n",
    "    if str(i) == 'nan':\n",
    "        stanza_tag.append('')\n",
    "    else:\n",
    "        stanza_tag.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d6b8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza_en_tag = []\n",
    "stanza_stanza_tag = []\n",
    "for i in range(len(en_tag)):\n",
    "    if stanza_tag == 'MISC' and en_tag[i] != '':\n",
    "        stanza_en_tag.append(en_tag[i])\n",
    "        stanza_stanza_tag.append(en_tag[i])\n",
    "    elif en_tag[i] == 'GPE':\n",
    "        stanza_en_tag.append('LOC')\n",
    "        stanza_stanza_tag.append(stanza_tag[i])\n",
    "    elif en_tag[i] != '' and stanza_tag[i] != '' and en_tag[i] != 'FAC' and en_tag[i] != 'VEH':\n",
    "        stanza_en_tag.append(en_tag[i])\n",
    "        stanza_stanza_tag.append(stanza_tag[i])\n",
    "    elif (en_tag[i] == '' and stanza_tag[i] != '') or (en_tag[i] != '' and stanza_tag[i] == '' and en_tag[i] != 'FAC' and en_tag[i] != 'VEH'):\n",
    "        stanza_en_tag.append(en_tag[i])\n",
    "        stanza_stanza_tag.append(stanza_tag[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711175c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(stanza_en_tag, stanza_stanza_tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afaa00e",
   "metadata": {},
   "source": [
    "### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c420d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tag = []\n",
    "for i in df_alltags['bert']:\n",
    "    if str(i) == 'nan':\n",
    "        bert_tag.append('')\n",
    "    else:\n",
    "        bert_tag.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd4f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_en_tag = []\n",
    "bert_bert_tag = []\n",
    "for i in range(len(en_tag)):\n",
    "    if en_tag[i] == 'GPE':\n",
    "        bert_en_tag.append('LOC')\n",
    "        bert_bert_tag.append(bert_tag[i])\n",
    "    elif en_tag[i] != '' and bert_tag[i] != '' and en_tag[i] != 'FAC' and en_tag[i] != 'VEH':\n",
    "        bert_en_tag.append(en_tag[i])\n",
    "        bert_bert_tag.append(bert_tag[i])\n",
    "    elif (en_tag[i] == '' and bert_tag[i] != '') or (en_tag[i] != '' and bert_tag[i] == '' and en_tag[i] != 'FAC' and en_tag[i] != 'VEH'):\n",
    "        bert_en_tag.append(en_tag[i])\n",
    "        bert_bert_tag.append(bert_tag[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c688160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(bert_en_tag, bert_bert_tag))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
